# -*- coding: utf-8 -*-
"""LM_Final_Project_BrainTumor_Tabular_Colab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17h0o3XXGpaOJXgvmzEzm0K66Q5kemgO3

# Final Project — Medical Tabular Classification (Brain Tumor + Breast Cancer)

**Author:** Leonardo Machado  
**Course:** Stanford Continuing Studies — Applied ML with Python  
**Date:** 2025-08-29

1. Project Title, Problem Statement, Objectives & Goals  
2. Methodology & Block Diagram (pipeline)  
3. Datasets (Brain Tumor)  
4. Data Visualization & Feature Engineering notes  
5. 8 Algorithms:
  5.1) LogReg,
  5.2) SVM-linear
  5.3) SVM-RBF
  5.4) kNN
  5.5) DecisionTree
  5.6) RandomForest
  5.6) GradientBoosting
  5.7) MLP
6. Experiments, Metrics, Comparison Tables/Graphs  
7. Optimization & Test Results
8. Standards/Constraints, Bias/Ethics, Limitations, Conclusion & Future Work

## 0) Setup
Reproducibility, imports, and utility functions.
"""

import numpy as np, pandas as pd, matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import (roc_auc_score, accuracy_score, f1_score, recall_score,
                             precision_score, confusion_matrix, classification_report,
                             RocCurveDisplay, ConfusionMatrixDisplay)
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
import tensorflow as tf
from tensorflow.keras import models, layers
from sklearn.datasets import load_breast_cancer
import os

#Garantee reproducibility
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)
tf.random.set_seed(RANDOM_STATE)

#Setting matplotlib styling
plt.rcParams['figure.figsize'] = (6,4)
plt.rcParams['axes.grid'] = True
plt.rcParams['grid.alpha'] = 0.6
plt.rcParams['grid.linestyle'] = '--'

"""## 1) Project framing
**Problem Statement:** Tumor detection from image-derived tabular features to support triage.

**Objectives & Goals:**
- Train **8 algorithms** (mix of classical ML + shallow DL).
- Prioritize **Recall (tumor class)** and **ROC-AUC**; include interpretability (coefficients/feature importances).

**Methodology / Pipeline:** Preprocess data → Split (test/validation data) → Pipeline(Scaler→Model) → CV + GridSearch → Test → Plots → Analysis.

## 2) Load Datasets
We use your Brain Tumor CSV and a comparator dataset (Breast Cancer Wisconsin) from scikit-learn.

**Instructions for Colab:**
1. Upload `Brain Tumor.csv` to Colab root (`/content`).
2. If not uploaded, run the upload cell below.
"""

from google.colab import files
#If the file is not already uploaded, upload the dataset
if not os.path.exists('/content/Brain Tumor.csv'):
    print('Upload Brain Tumor.csv...')
    #Upload file
    uploaded = files.upload()

#Define dataset path
bt_path = '/content/Brain Tumor.csv'
#Read dataset into bt variable
bt = pd.read_csv(bt_path)

# Features: drop non-feature column 'Image' and keep 'Class' as target if present
X_bt = bt.drop(columns=[c for c in ['Image','Class'] if c in bt.columns])
y_bt = bt['Class'].astype(int)

# Comparator dataset
bc = load_breast_cancer()
X_bc = pd.DataFrame(bc.data, columns=bc.feature_names)
y_bc = pd.Series(bc.target, name='Class')

print('Brain Tumor shape:', X_bt.shape, ' target n:', y_bt.shape)
print('Breast Cancer shape:', X_bc.shape, ' target n:', y_bc.shape)

# Class balance quick look
print('\nBrain Tumor class counts:\n', y_bt.value_counts())
print('\nBreast Cancer class counts:\n', y_bc.value_counts())

"""## 2.1) Dataset overview"""

import numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns
from sklearn.feature_selection import mutual_info_classif

plt.rcParams["figure.figsize"] = (7,5)
plt.rcParams["axes.grid"] = True
plt.rcParams["grid.alpha"] = 0.4
plt.rcParams["grid.linestyle"] = "--"

def savefig(name):
    plt.tight_layout()
    plt.savefig(name, dpi=300, bbox_inches="tight")
    plt.show()

def top_features_by_variance(X, k=6):
    v = X.var().sort_values(ascending=False)
    return list(v.head(k).index)

def top_features_by_mi(X, y, k=6):
    # Safe MI even if some cols constant
    X_ = X.select_dtypes(include=[np.number]).fillna(0.0)
    mi = mutual_info_classif(X_, y, discrete_features=False, random_state=42)
    mi = pd.Series(mi, index=X_.columns).sort_values(ascending=False)
    return list(mi.head(k).index)

"""# 2.1.1) Brain Tumor"""

bt_df = pd.concat([X_bt.copy(), y_bt.rename("Class")], axis=1)

# Peek at the data
display(bt_df.head(10))           # first 10 rows
display(bt_df.describe().T)       # stats summary

# dtypes summary
print("Dtype counts:\n", bt_df.dtypes.value_counts(), "\n")

# class distribution
cls_counts = bt_df["Class"].value_counts().sort_index()
ax = cls_counts.plot(kind="bar")
ax.set_title("Brain Tumor — Class Distribution")
ax.set_xlabel("Class (0 = no tumor, 1 = tumor)")
ax.set_ylabel("Count")
savefig("brain_class_distribution.png")




# Pick a small feature set to visualize (variance or mutual information)
bt_feats_var = top_features_by_variance(X_bt, k=6)
try:
    bt_feats_mi  = top_features_by_mi(X_bt, y_bt, k=6)
except Exception:
    bt_feats_mi = bt_feats_var

bt_feats = list(dict.fromkeys(bt_feats_var + bt_feats_mi))[:6]  # merge & keep first 6

# Histograms by class
for f in bt_feats:
    plt.figure()
    for c in sorted(bt_df["Class"].unique()):
        subset = bt_df.loc[bt_df["Class"]==c, f].values
        plt.hist(subset, bins=30, alpha=0.5, label=f"Class {c}", density=True)
    plt.title(f"Brain Tumor — Feature Distribution by Class: {f}")
    plt.xlabel(f)
    plt.ylabel("Density")
    plt.legend()
    savefig(f"brain_hist_{f}.png")

# Correlation heatmap (features only)
plt.figure(figsize=(7.5,6))
corr = X_bt.corr(numeric_only=True)
sns.heatmap(corr, cmap="vlag", center=0, square=True, cbar_kws={"shrink": .8})
plt.title("Brain Tumor — Feature Correlation Heatmap")
savefig("brain_corr_heatmap.png")

"""# 2.1.2) Breast Cancer"""

bc_df = pd.concat([X_bc.copy(), y_bc.rename("Class")], axis=1)

# Peek at the data
display(bc_df.head(10))
display(bc_df.describe().T)

# dtypes summary
print("Dtype counts:\n", bc_df.dtypes.value_counts(), "\n")

# class distribution
cls_counts = bc_df["Class"].value_counts().sort_index()
ax = cls_counts.plot(kind="bar")
ax.set_title("Breast Cancer — Class Distribution")
ax.set_xlabel("Class (0 = benign, 1 = malignant)")
ax.set_ylabel("Count")
savefig("breast_class_distribution.png")




# Select features to visualize
bc_feats_var = top_features_by_variance(X_bc, k=6)
try:
    bc_feats_mi  = top_features_by_mi(X_bc, y_bc, k=6)
except Exception:
    bc_feats_mi = bc_feats_var

bc_feats = list(dict.fromkeys(bc_feats_var + bc_feats_mi))[:6]

# Histograms by class
for f in bc_feats:
    plt.figure()
    for c in sorted(bc_df["Class"].unique()):
        subset = bc_df.loc[bc_df["Class"]==c, f].values
        plt.hist(subset, bins=30, alpha=0.5, label=f"Class {c}", density=True)
    plt.title(f"Breast Cancer — Feature Distribution by Class: {f}")
    plt.xlabel(f)
    plt.ylabel("Density")
    plt.legend()
    savefig(f"breast_hist_{f}.png")

# Correlation heatmap
plt.figure(figsize=(7.5,6))
corr = X_bc.corr(numeric_only=True)
sns.heatmap(corr, cmap="vlag", center=0, square=True, cbar_kws={"shrink": .8})
plt.title("Breast Cancer — Feature Correlation Heatmap")
savefig("breast_corr_heatmap.png")

"""## 3) Train/Test Split"""



from sklearn.model_selection import train_test_split


def prep_split(X, y, test_size=0.2, seed=RANDOM_STATE):
    # Stratify preserves label ratios in train vs test
    return train_test_split(X, y, test_size=test_size, stratify=y, random_state=seed)

X_bt_tr, X_bt_te, y_bt_tr, y_bt_te = prep_split(X_bt, y_bt)
X_bc_tr, X_bc_te, y_bc_tr, y_bc_te = prep_split(X_bc, y_bc)

print('BT train/test:', X_bt_tr.shape, X_bt_te.shape)
print('BC train/test:', X_bc_tr.shape, X_bc_te.shape)

"""## 4) Models & Hyperparameter Grids (8 total)
We’ll use GridSearchCV (StratifiedKFold=5) and ROC-AUC as the performance metric.
"""

from sklearn.model_selection import StratifiedKFold, GridSearchCV

#A dictionary models_grids with 7 Machine Learning Methods
#Pipeline chain pre-processing and the model together
#StandardScaler to standardize features -> especially important for distance-based algorithms

models_grids = {
    'LogReg': (
        Pipeline([('scaler', StandardScaler()),
                  ('clf', LogisticRegression(max_iter=1000, class_weight='balanced', random_state=RANDOM_STATE))]),
        {'clf__C':[0.1,1,10]}
    ),
    'SVM-Linear': (
        Pipeline([('scaler', StandardScaler()),
                  ('clf', SVC(kernel='linear', probability=True, class_weight='balanced', random_state=RANDOM_STATE))]),
        {'clf__C':[0.1,1,10]}
    ),
    'SVM-RBF': (
        Pipeline([('scaler', StandardScaler()),
                  ('clf', SVC(kernel='rbf', probability=True, class_weight='balanced', random_state=RANDOM_STATE))]),
        {'clf__C':[0.5,1,5], 'clf__gamma':['scale','auto']}
    ),
    'kNN': (
        Pipeline([('scaler', StandardScaler()),
                  ('clf', KNeighborsClassifier())]),
        {'clf__n_neighbors':[3,5,7,9]}
    ),
    'DecisionTree': (
        DecisionTreeClassifier(max_depth=None, class_weight='balanced', random_state=RANDOM_STATE),
        {'max_depth':[None,3,5,7], 'min_samples_leaf':[1,3,5]}
    ),
    'RandomForest': (
        RandomForestClassifier(n_estimators=200, class_weight='balanced', random_state=RANDOM_STATE, n_jobs=-1),
        {'max_depth':[None,5,10], 'min_samples_leaf':[1,3,5]}
    ),
    'GradientBoosting': (
        GradientBoostingClassifier(random_state=RANDOM_STATE),
        {'learning_rate':[0.05,0.1], 'n_estimators':[150,250], 'max_depth':[2,3]}
    ),
    #Multilayer Perceptron
    'MLP': (
        Pipeline([('scaler', StandardScaler()), ('clf', None)]),
        {}
    )
}

def cv_fit_best(name, estimator, param_grid, Xtr, ytr):
    # Training set is split into 5 folds.
    # StratifiedKFold ensures each fold has the same class balance as the whole dataset.
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)
    # Define a dictionary of cadidate values for each hyperparameter
    # Tries every combination
    # Utilizes ROC_AUC as performance metric. Measures how well the model ranks positive cases above negative cases.
    # Refit=true retrains a fresh model on the entire training set using the best hyperparameters
    gs = GridSearchCV(estimator, param_grid, scoring='roc_auc', cv=cv, n_jobs=-1, refit=True)
    gs.fit(Xtr, ytr)
    return gs.best_estimator_, gs.best_score_, gs.best_params_

def evaluate_on_test(model, Xte, yte):
    y_pred = model.predict(Xte)
    try:
        y_proba = model.predict_proba(Xte)[:,1]
    except Exception:
        dec = model.decision_function(Xte)
        y_proba = (dec - dec.min()) / (dec.max() - dec.min() + 1e-9)
    metrics = {
        'accuracy': accuracy_score(yte, y_pred),
        'precision': precision_score(yte, y_pred, zero_division=0),
        'recall': recall_score(yte, y_pred, zero_division=0),
        'f1': f1_score(yte, y_pred, zero_division=0),
        'roc_auc': roc_auc_score(yte, y_proba),
    }
    return metrics, y_pred, y_proba

"""## 5) Keras MLP (shallow DL)
Feedforward network to complete the 8 algorithms.
"""

# Default number of neurons in the hidden layers = 64
# Dropout rate to reduce overfitting = 10%
def build_mlp(input_dim, hidden=64, dropout=0.1):
    m = models.Sequential([
        layers.Input(shape=(input_dim,)),
        layers.Dense(hidden, activation='relu'),
        layers.Dropout(dropout),
        layers.Dense(1, activation='sigmoid')
    ])
    #Sigmoid activation function so output is between 0 and 1
    m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return m

def run_mlp(Xtr, ytr, Xte, yte, epochs=40, batch=32):
    # Build MLP
    mlp = build_mlp(Xtr.shape[1])
    # Standardize trainning and test data
    scaler = StandardScaler().fit(Xtr)
    Xtr_s = scaler.transform(Xtr)
    Xte_s = scaler.transform(Xte)
    # If model does not improve for 5 epochs, model stops early
    es = tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True, monitor='val_loss')
    hist = mlp.fit(Xtr_s, ytr, validation_split=0.2, epochs=epochs, batch_size=batch, verbose=0, callbacks=[es])
    y_proba = mlp.predict(Xte_s, verbose=0).ravel()
    y_pred = (y_proba >= 0.5).astype(int)
    metrics = {
        'accuracy': accuracy_score(yte, y_pred),
        'precision': precision_score(yte, y_pred, zero_division=0),
        'recall': recall_score(yte, y_pred, zero_division=0),
        'f1': f1_score(yte, y_pred, zero_division=0),
        'roc_auc': roc_auc_score(yte, y_proba),
    }
    return mlp, scaler, hist.history, metrics, y_pred, y_proba

"""## 6) Run Experiment Suite on Both Datasets
Trains/tunes classical models with CV, runs MLP, collects results, and prints comparison tables.
"""

def run_suite(dataset_name, Xtr, ytr, Xte, yte):
    rows = []
    fitted = {}
    for name, (est, grid) in models_grids.items():
        if name != 'MLP':
            best_model, cv_auc, best_params = cv_fit_best(name, est, grid, Xtr, ytr)
            test_metrics, y_pred, y_proba = evaluate_on_test(best_model, Xte, yte)
            rows.append({'Dataset': dataset_name, 'Model': name, 'CV_ROC_AUC': cv_auc, **test_metrics, 'BestParams': best_params})
            fitted[name] = {'model': best_model, 'y_pred': y_pred, 'y_proba': y_proba}
        else:
            mlp, scaler, history, test_metrics, y_pred, y_proba = run_mlp(Xtr, ytr, Xte, yte)
            rows.append({'Dataset': dataset_name, 'Model': 'MLP', 'CV_ROC_AUC': np.nan, **test_metrics, 'BestParams': {'hidden':64,'dropout':0.1}})
            fitted['MLP'] = {'model': mlp, 'scaler': scaler, 'history': history, 'y_pred': y_pred, 'y_proba': y_proba}
    df = pd.DataFrame(rows).sort_values(['Dataset','roc_auc'], ascending=[True,False]).reset_index(drop=True)
    return df, fitted

bt_results, bt_fitted = run_suite('BrainTumor', X_bt_tr, y_bt_tr, X_bt_te, y_bt_te)
bc_results, bc_fitted = run_suite('BreastCancer', X_bc_tr, y_bc_tr, X_bc_te, y_bc_te)

print('Brain Tumor results:')
display(bt_results)
print('\nBreast Cancer results:')
display(bc_results)

# --- Install/Imports (seaborn is optional but makes clean bars) ---
# If seaborn isn't installed in your Colab runtime, uncomment:
# !pip install seaborn

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

sns.set_context("talk")
sns.set_style("whitegrid")
plt.rcParams["figure.figsize"] = (10, 6)
plt.rcParams["savefig.dpi"] = 300

# ---------- Helpers ----------
def _save(fig, fname):
    fig.tight_layout()
    fig.savefig(fname, bbox_inches="tight")
    print(f"Saved: {fname}")

def _prep_single(df, dataset, metric):
    d = df.copy()
    d = d.sort_values(metric, ascending=False)
    return d

def barplot_single(results_df, dataset_name, metric="f1", filename=None, ylim=None, decimals=3):
    """
    Make a bar plot for one dataset, one metric (e.g., f1 or roc_auc).
    """
    d = _prep_single(results_df, dataset_name, metric)
    fig, ax = plt.subplots()
    ax = sns.barplot(data=d, x="Model", y=metric, color="C0", edgecolor="black")
    ax.set_title(f"{dataset_name} — Model Comparison ({metric.upper()})")
    ax.set_xlabel("Model")
    ax.set_ylabel(metric.upper())
    ax.set_ylim(0 if ylim is None else ylim[0], 1 if ylim is None else ylim[1])
    # Annotate bars
    for p in ax.patches:
        val = p.get_height()
        ax.annotate(f"{val:.{decimals}f}", (p.get_x() + p.get_width()/2, val),
                    ha="center", va="bottom", fontsize=10, xytext=(0,5), textcoords="offset points")
    plt.xticks(rotation=25, ha="right")
    plt.tight_layout()
    if filename:
        _save(fig, filename)
    plt.show()

def barplot_grouped(bt_df, bc_df, metric="f1", filename=None, decimals=3):
    """
    Grouped bars (BrainTumor vs BreastCancer) per model for one metric.
    Assumes both tables contain the same set of models (order will follow BrainTumor).
    """
    a = bt_df[["Model", metric]].copy()
    a["Dataset"] = "BrainTumor"
    b = bc_df[["Model", metric]].copy()
    b["Dataset"] = "BreastCancer"
    g = pd.concat([a, b], ignore_index=True)

    # Keep a consistent model order based on BrainTumor ranking
    order = bt_df.sort_values(metric, ascending=False)["Model"].tolist()
    fig, ax = plt.subplots(figsize=(12,6))
    ax = sns.barplot(data=g, x="Model", y=metric, hue="Dataset", order=order, edgecolor="black")
    ax.set_title(f"Model Comparison Across Datasets ({metric.upper()})")
    ax.set_xlabel("Model")
    ax.set_ylabel(metric.upper())
    # Annotate bars
    for p in ax.patches:
        val = p.get_height()
        ax.annotate(f"{val:.{decimals}f}", (p.get_x() + p.get_width()/2, val),
                    ha="center", va="bottom", fontsize=9, xytext=(0,4), textcoords="offset points")
    plt.xticks(rotation=25, ha="right")
    plt.legend(title="Dataset")
    plt.tight_layout()
    if filename:
        _save(fig, filename)
    plt.show()

# ---------- Make the plots ----------
# Ensure the expected columns exist:
assert {"Model","f1","roc_auc"}.issubset(bt_results.columns)
assert {"Model","f1","roc_auc"}.issubset(bc_results.columns)

# 1) Single-dataset bars (F1)
barplot_single(bt_results, "Brain Tumor", metric="f1", filename="BT_model_comparison_F1.png")
barplot_single(bc_results, "Breast Cancer", metric="f1", filename="BC_model_comparison_F1.png")

# 2) Single-dataset bars (ROC-AUC)
barplot_single(bt_results, "Brain Tumor", metric="roc_auc", filename="BT_model_comparison_AUC.png")
barplot_single(bc_results, "Breast Cancer", metric="roc_auc", filename="BC_model_comparison_AUC.png")

# 3) Grouped bars across datasets (same metric)
barplot_grouped(bt_results, bc_results, metric="f1", filename="Grouped_model_comparison_F1.png")
barplot_grouped(bt_results, bc_results, metric="roc_auc", filename="Grouped_model_comparison_AUC.png")

"""## 7) Plots — Best Model per Dataset
Confusion Matrix and ROC Curve using the recorded predictions.
"""

def best_by_auc(results_df):
    # Drop rows where roc_auc is NaN
    valid_df = results_df.dropna(subset=["roc_auc"])
    if valid_df.empty:
        return None  # or raise an Exception if you prefer
    return valid_df.loc[valid_df['roc_auc'].idxmax(), 'Model']

def plot_test_curves(dataset_name, results_df, fitted, Xte, yte):
    best_model_name = best_by_auc(results_df)
    print(f'Best model name= {best_model_name}')
    y_pred = fitted[best_model_name]['y_pred']
    y_proba = fitted[best_model_name]['y_proba']

    # Confusion Matrix
    disp = ConfusionMatrixDisplay.from_predictions(yte, y_pred)
    plt.title(f"{dataset_name} — Confusion Matrix ({best_model_name})")
    plt.show()

    # ROC Curve
    RocCurveDisplay.from_predictions(yte, y_proba)
    plt.title(f"{dataset_name} — ROC Curve ({best_model_name})")
    plt.show()

plot_test_curves('BrainTumor', bt_results, bt_fitted, X_bt_te, y_bt_te)
plot_test_curves('BreastCancer', bc_results, bc_fitted, X_bc_te, y_bc_te)